{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> <font color = \"teal\"> Tareas:</font> </u>\n",
    "## <font color = \"teal\"> * Detectar archivos con problemas de georrefercia </font> \n",
    "## <font color = \"teal\"> * Implementar procesos de renombre </font> \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"lime\"> .csv de Argendata: cada uno de sus encodings y delimiters </font>\n",
    "\n",
    "No todos los .csv que contempla Argendata tienen el mismo enconding y delimiter. Por eso antes de comenzar a trabajar voy a armar el siguiente input para la tarea: dataframe que contenga 4 columnas que sean 'TOPICO', 'archivo_csv', 'encoding' y delimiter'.\n",
    "\n",
    "Me gustaría algo explicito que por cada carpeta (topico) dentro de /home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata encuentre por cada uno de sus .csv el encoding y delimiter específico para abrir cada .csv. Y que el resultado final sea un dataframe que contenga 4 columnas que sean 'TOPICO', 'archivo_csv', 'encoding' y delimiter'.\n",
    "\n",
    "### <font color = \"red\"> ¿Meter este proceso en QA? Para ya tener el respectivo encoding delimiter de cada.csv </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import chardet\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# ruta de tabajo\n",
    "base_dir = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata'\n",
    "\n",
    "# lista para recolectar los datos\n",
    "data = []\n",
    "\n",
    "# tamanio del fragmento a leer (en este caso 100000 bytes)\n",
    "sample_size = 100000\n",
    "\n",
    "for topico in os.listdir(base_dir): # ejemplo SEBACO\n",
    "    topico_path = os.path.join(base_dir, topico) #ejemplo /home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata/SEBACO\n",
    "\n",
    "    # ignorar archivos sueltos como LICENSE y README.md\n",
    "    if not os.path.isdir(topico_path):\n",
    "        continue\n",
    "\n",
    "    for archivo in os.listdir(topico_path):\n",
    "        if archivo.endswith('.csv'):\n",
    "            archivo_path = os.path.join(topico_path, archivo)\n",
    "\n",
    "            # leer un fragmento del .csv para análisis\n",
    "            with open(archivo_path, 'rb') as f:\n",
    "                raw_sample = f.read(sample_size)\n",
    "\n",
    "            # detectar encoding\n",
    "            encoding_detected = 'utf-8'  # por defecto\n",
    "            try:\n",
    "                encoding_info = chardet.detect(raw_sample)\n",
    "                if encoding_info and encoding_info['encoding']:\n",
    "                    encoding_detected = encoding_info['encoding']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # decodificar el fragmento\n",
    "            try:\n",
    "                sample = raw_sample.decode(encoding_detected, errors='replace')\n",
    "            except:\n",
    "                sample = raw_sample.decode('utf-8', errors='replace') \n",
    "\n",
    "            # detectar delimitador\n",
    "            delimiter_detected = ','\n",
    "            try:\n",
    "                sniffer = csv.Sniffer()\n",
    "                dialect = sniffer.sniff(sample)\n",
    "                delimiter_detected = dialect.delimiter\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # agregar al resultado\n",
    "            data.append({\n",
    "                'TOPICO': topico,\n",
    "                'archivo_csv': archivo,\n",
    "                'encoding': encoding_detected,\n",
    "                'delimiter': delimiter_detected\n",
    "            })\n",
    "\n",
    "# crear el DataFrame\n",
    "df_resultado = pd.DataFrame(data)\n",
    "\n",
    "# mostrar resultado\n",
    "# print(df_resultado)\n",
    "\n",
    "# guardar como csv el archivo csvTopicoArgendata_enciding_and_delimiters\n",
    "# path_df_encoding_delimiters = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/'\n",
    "# df_resultado.to_csv(path_df_encoding_delimiters + 'csvTopicosArgendata_codificacion_y_delimitadores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tengo 'csvTopicosArgendata_codificacion_y_delimitadores.csv, voy a enfocarme en aquellas columnas que tengan datos de georreferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahora que ya tengo todos los csv de todos los tópicos de Argendata/data junto con sus columnas, voy a enfocarme en aquellas que tengan información de georreferencia. Para eso voy a usar dos inputs dados y algunas consideraciones:\n",
    "- Joan me pasó columnas que hasta el momento se sabe que son de georef -> georef_keys.txt\n",
    "- voy a usar el geonomeclador de argendata, la pestaña curada export (geonomeclador_export.csv)\n",
    "- voy a tener en cuenta solo aquellas comunnas de georeferencia en español: geonomeclador_export.csv para lo que es las columnas \"name_long\" y \"name_short\" solo considera el español\n",
    "- de las columnas a investigar y buscar si corresponden a georeferencia, voy a quedarme con aquellas que: el 80% de su contenido aparezca en cada csv que busco\n",
    "\n",
    "Que voy a considerar como observaciones:\n",
    "- aquellas columnas, de sus respectivos .csv y tópicos que no estén en español\n",
    "- aquellos nombres/códigos que deberían ser cierta \"nombre/codigo\" y no lo son"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
